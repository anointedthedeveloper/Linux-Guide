# robots.txt for Linux Helper
# This file tells search engines which pages to crawl and index

User-agent: *
Allow: /
Allow: /installation
Allow: /terminal
Allow: /errors
Allow: /troubleshooting
Allow: /privacy
Allow: /cookies
Allow: /adsense
Allow: /about
Allow: /contact

# Disallow private or temporary pages
Disallow: /api/
Disallow: /.next/
Disallow: /node_modules/

# Sitemap location
Sitemap: https://linuxhelper.dev/sitemap.xml

# Crawl delay for respectful crawling
Crawl-delay: 1
